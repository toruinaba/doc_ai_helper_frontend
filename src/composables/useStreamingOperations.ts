import { ref, type Ref } from 'vue'
import type { components } from '@/services/api/types.auto'
import { useDocumentStore } from '@/stores/document.store'
import { llmService } from '@/services/api/llm.service'
import { useAsyncOperation } from '@/composables/useAsyncOperation'
import { getDefaultRepositoryConfig } from '@/utils/config.util'
import type { ClientMessage } from '@/composables/useMessageManagement'

type DocumentResponse = components['schemas']['DocumentResponse']
type MessageItem = components['schemas']['MessageItem']

export interface DocumentContextConfig {
  enableRepositoryContext?: boolean
  enableDocumentMetadata?: boolean
  includeDocumentInSystemPrompt?: boolean
  systemPromptTemplate?: string
}

export interface StreamingCallbacks {
  onStart?: () => void
  onChunk?: (chunk: string) => void
  onToolCall?: (toolCall: any) => void
  onToolResult?: (result: any) => void
  onError?: (error: Error) => void
  onEnd?: (data?: any) => void
}

export interface StreamingOperations {
  isStreamingWithTools: Ref<boolean>
  currentStreamController: Ref<AbortController | (() => void) | null>
  sendStreamingMessage: (content: string) => Promise<void>
  sendStreamingMessageWithConfig: (content: string, config?: Partial<DocumentContextConfig>) => Promise<void>
  sendStreamingMessageWithToolsAndConfig: (
    content: string, 
    config?: Partial<DocumentContextConfig>,
    onToken?: (token: string) => void,
    forceToolChoice?: string
  ) => Promise<void>
  stopStreaming: () => void
}

export function useStreamingOperations(
  messages: Ref<ClientMessage[]>,
  addUserMessage: (content: string) => ClientMessage,
  addAssistantMessage: (content: string) => ClientMessage,
  addSystemMessage: (content: string) => ClientMessage,
  generateMessageId: () => string,
  mcpToolsConfig: Ref<any>,
  shouldUseMCPTools: (content: string, autoDetect: boolean) => { recommended: boolean; reasons: string[] },
  activeToolExecutions: Ref<any[]>,
  startToolExecution: (toolCall: any) => any,
  updateToolExecutionStatus: (executionId: string, status: string, result?: any) => void
): StreamingOperations {
  const documentStore = useDocumentStore()
  const asyncOp = useAsyncOperation()
  
  const isStreamingWithTools = ref(false)
  const currentStreamController = ref<AbortController | (() => void) | null>(null)

  // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§LLMã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡
  async function sendStreamingMessage(content: string) {
    console.log('Start sending streaming message:', content)
    
    try {
      // ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
      addUserMessage(content)
      
      // ç¾åœ¨ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
      const currentDoc = documentStore.currentDocument
      
      if (!currentDoc) {
        throw new Error('ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“')
      }
      
      // ãƒªãƒã‚¸ãƒˆãƒªæƒ…å ±ã‚’å–å¾— (ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¹ãƒˆã‚¢ã®å€¤ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã—ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨)
      const defaultConfig = getDefaultRepositoryConfig()
      const service = documentStore.currentService || defaultConfig.service
      const owner = documentStore.currentOwner || defaultConfig.owner
      const repo = documentStore.currentRepo || defaultConfig.repo
      const path = documentStore.currentPath || defaultConfig.path
      const ref = documentStore.currentRef || defaultConfig.ref
      
      // ä¼šè©±å±¥æ­´ã®æº–å‚™ï¼ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’APIã®å½¢å¼ã«å¤‰æ›ï¼‰
      const conversationHistory = messages.value.map(msg => ({
        role: msg.role,
        content: msg.content,
        timestamp: msg.timestamp.toISOString()
      }))
      
      console.log('Sending streaming chat message with conversation history:', conversationHistory.length, 'messages')
      
      // ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ç©ºãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºç”¨ï¼‰
      const assistantMessage = addAssistantMessage('')
      let accumulatedContent = ''
      
      // æ–°ã—ã„çµ±ä¸€ã‚µãƒ¼ãƒ“ã‚¹ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
      await llmService.stream(
        {
          prompt: content,
          provider: 'openai',
          conversationHistory,
          includeDocument: true,
          enableTools: false,
          toolChoice: 'none'
        },
        currentDoc,
        {
          onStart: () => {
            console.log('Streaming started')
          },
          onChunk: (chunk) => {
            console.log('Received chunk in streaming operations:', chunk)
            // æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ã‚’å—ä¿¡ã—ãŸã‚‰ç´¯ç©ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«è¿½åŠ 
            accumulatedContent += chunk
            console.log('Accumulated content so far:', accumulatedContent)
            
            // ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ›´æ–°
            const messageIndex = messages.value.findIndex(m => m.id === assistantMessage.id)
            console.log('Found message at index:', messageIndex, 'with ID:', assistantMessage.id)
            if (messageIndex !== -1) {
              // Vueã®ãƒªã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã‚’ç¢ºå®Ÿã«ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹ãŸã‚ã€æ–°ã—ã„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
              const updatedMessage = {
                ...messages.value[messageIndex],
                content: accumulatedContent
              }
              messages.value[messageIndex] = updatedMessage
              console.log('Updated message content:', messages.value[messageIndex].content)
            } else {
              console.warn('Could not find message to update')
            }
          },
          onError: (error) => {
            console.error('Streaming error:', error)
            const errorMsg = error.message || 'Unknown streaming error'
            
            // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            addSystemMessage(`ã‚¨ãƒ©ãƒ¼: ${errorMsg}`)
          },
          onEnd: (data) => {
            console.log('Streaming completed:', data)
            
            // æœ€é©åŒ–ã•ã‚ŒãŸä¼šè©±å±¥æ­´ã®å‡¦ç†ã‚’æ”¹å–„
            if (data?.optimized_conversation_history && data.optimized_conversation_history.length > 0) {
              console.log('Using optimized conversation history from the server:', 
                data.optimized_conversation_history.length, 'messages')
              
              // ç¾åœ¨ã®ä¼šè©±å±¥æ­´ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
              const oldMessages = [...messages.value]
              console.log('Previous messages count:', oldMessages.length)
              
              // MCPãƒ„ãƒ¼ãƒ«ä½¿ç”¨æ™‚ã¯ç¾åœ¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿æŒã—ã€æœ€é©åŒ–ã¯é€éçš„ã«å‡¦ç†
              // é€šå¸¸ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ã®ã¿å±¥æ­´ã‚’ç½®ãæ›ãˆã‚‹
              console.log('Replacing conversation history with optimized version for regular streaming')
              
              // æ—¢å­˜ã®ä¼šè©±å±¥æ­´ã‚’ã‚¯ãƒªã‚¢
              messages.value = []
              
              // æœ€é©åŒ–ã•ã‚ŒãŸä¼šè©±å±¥æ­´ã‚’è¿½åŠ 
              data.optimized_conversation_history.forEach((msg: MessageItem, index: number) => {
                console.log(`Adding message ${index} with role ${msg.role}`)
                const clientMsg: ClientMessage = {
                  id: generateMessageId(),
                  role: msg.role as 'user' | 'assistant' | 'system',
                  content: msg.content,
                  timestamp: msg.timestamp ? new Date(msg.timestamp) : new Date()
                }
                messages.value.push(clientMsg)
              })
              console.log('Updated messages after optimization:', messages.value.length)
            }
          }
        }
      )
      
      // æ–°ã—ã„ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè£…ã§ã¯ä¸­æ–­æ©Ÿèƒ½ã¯å†…éƒ¨ã§ç®¡ç†ã•ã‚Œã‚‹
      console.log('Streaming chat message sent successfully')
    } catch (error) {
      console.error('Error in streaming message:', error)
      const errorMsg = error instanceof Error ? error.message : 'Unknown error'
      addSystemMessage(`ã‚¨ãƒ©ãƒ¼: ${errorMsg}`)
    }
  }

  // æ–°ã—ã„ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ä»•æ§˜ã«å¯¾å¿œã—ãŸã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡
  async function sendStreamingMessageWithConfig(content: string, config?: Partial<DocumentContextConfig>) {
    console.log('Start sending streaming message with config:', content, config)
    
    try {
      // ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
      addUserMessage(content)
      
      // ç¾åœ¨ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
      const currentDoc = documentStore.currentDocument
      
      if (!currentDoc) {
        throw new Error('ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“')
      }
      
      // ãƒªãƒã‚¸ãƒˆãƒªæƒ…å ±ã‚’å–å¾— (ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¹ãƒˆã‚¢ã®å€¤ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã—ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨)
      const defaultConfig = getDefaultRepositoryConfig()
      const service = documentStore.currentService || defaultConfig.service
      const owner = documentStore.currentOwner || defaultConfig.owner
      const repo = documentStore.currentRepo || defaultConfig.repo
      const path = documentStore.currentPath || defaultConfig.path
      const ref = documentStore.currentRef || defaultConfig.ref
      
      // è¨­å®šã®çµ±åˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¼ã‚¸ï¼‰
      const effectiveConfig = {
        enableRepositoryContext: config?.enableRepositoryContext ?? true,
        enableDocumentMetadata: config?.enableDocumentMetadata ?? true,
        includeDocumentInSystemPrompt: config?.includeDocumentInSystemPrompt ?? true,
        systemPromptTemplate: config?.systemPromptTemplate ?? 'contextual_document_assistant_ja'
      }
      
      // ä¼šè©±å±¥æ­´ã®æº–å‚™ï¼ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’APIã®å½¢å¼ã«å¤‰æ›ï¼‰
      const conversationHistory = messages.value.map(msg => ({
        role: msg.role,
        content: msg.content,
        timestamp: msg.timestamp.toISOString()
      }))
      
      console.log('ğŸ“¤ Sending message request with new backend specification')
      
      // æ–°ã—ã„çµ±ä¸€ã‚µãƒ¼ãƒ“ã‚¹ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡
      const response = await llmService.query({
        prompt: content,
        provider: 'openai',
        conversationHistory,
        includeDocument: true
      }, currentDoc)
      
      // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å‡¦ç†
      if (response.content) {
        addAssistantMessage(response.content)
        
        // ä¼šè©±å±¥æ­´ã®æœ€é©åŒ–ãŒã‚ã£ãŸå ´åˆã¯é©ç”¨
        if (response.optimized_conversation_history && response.optimized_conversation_history.length > 0) {
          console.log('ğŸ—‚ï¸ Server provided optimized conversation history:', 
            response.optimized_conversation_history.length, 'messages')
        }
      }
      
      console.log('Message sent successfully with new specification')
      
    } catch (err) {
      const errorMessage = err instanceof Error ? err.message : 'Unknown error occurred'
      console.error('Error sending message with config:', err)
      
      // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
      addSystemMessage(`ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: ${errorMessage}`)
    }
  }

  // æ–°ã—ã„ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ä»•æ§˜ã«å¯¾å¿œã—ãŸMCPãƒ„ãƒ¼ãƒ«ä»˜ãã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡
  async function sendStreamingMessageWithToolsAndConfig(
    content: string, 
    config?: Partial<DocumentContextConfig>,
    onToken?: (token: string) => void,
    forceToolChoice?: string
  ) {
    console.log('Start streaming message with MCP tools and config:', content, config)
    isStreamingWithTools.value = true
    
    // å‰ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãŒã‚ã‚Œã°ä¸­æ­¢
    if (currentStreamController.value) {
      if (typeof currentStreamController.value === 'function') {
        currentStreamController.value()
      } else {
        currentStreamController.value.abort()
      }
    }
    
    try {
      // ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
      const userMessage = addUserMessage(content)
      
      // è¨­å®šã¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ãƒãƒ¼ã‚¸
      const effectiveConfig = {
        includeDocumentInSystemPrompt: true,
        systemPromptTemplate: 'contextual_document_assistant_ja',
        enableRepositoryContext: true,
        enableDocumentMetadata: true,
        completeToolFlow: true,
        ...config
      }
      
      console.log('ğŸ“‹ Streaming MCP tools with document context config:', effectiveConfig)
      
      // ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã®åˆ¤å®š
      const toolRecommendation = shouldUseMCPTools(content, mcpToolsConfig.value.autoDetect)
      const useTools = mcpToolsConfig.value.enabled && toolRecommendation.recommended
      
      // executionModeã«åŸºã¥ã„ã¦toolChoiceã‚’è¨­å®š
      let toolChoice: string
      if (forceToolChoice) {
        toolChoice = forceToolChoice
      } else if (useTools) {
        // MCPãƒ„ãƒ¼ãƒ«ãŒæœ‰åŠ¹ãªå ´åˆã¯ã€toolChoiceã‚’ä½¿ç”¨ï¼ˆ'auto', 'none', 'required', ã¾ãŸã¯ç‰¹å®šã®ãƒ„ãƒ¼ãƒ«åï¼‰
        toolChoice = mcpToolsConfig.value.toolChoice
      } else {
        toolChoice = 'none'
      }
      
      console.log('Streaming with tools - Tool recommendation:', toolRecommendation)
      console.log('Using tools:', useTools, 'Tool choice:', toolChoice)
      
      // ç¾åœ¨ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
      const currentDoc = documentStore.currentDocument
      
      if (!currentDoc) {
        throw new Error('ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“')
      }
      
      // ä¼šè©±å±¥æ­´ã‚’æ§‹ç¯‰
      const conversationHistory: MessageItem[] = messages.value.map(msg => ({
        role: msg.role,
        content: msg.content,
        timestamp: msg.timestamp.toISOString()
      }))
      
      // æ–°ã—ã„ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ä»•æ§˜ã«åˆã‚ã›ãŸå‡¦ç†ã‚’é–‹å§‹
      console.log('ğŸŒŠğŸ› ï¸ Preparing streaming MCP tools request:', {
        useTools,
        toolChoice
      })
      
      // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”¨ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
      const assistantMessage = addAssistantMessage('')
      let accumulatedContent = ''
      
      // æ–°ã—ã„çµ±ä¸€ã‚µãƒ¼ãƒ“ã‚¹ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
      await llmService.stream(
        {
          prompt: content,
          provider: 'openai',
          conversationHistory,
          includeDocument: true,
          enableTools: useTools,
          toolChoice,
          completeToolFlow: true
        },
        currentDoc,
        {
          onStart: () => {
            console.log('ğŸš€ MCP tools streaming started')
          },
          onChunk: (chunk) => {
            console.log('ğŸ¯ MCP tools chunk received:', chunk)
            accumulatedContent += chunk
            
            // ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ›´æ–°
            const messageIndex = messages.value.findIndex(m => m.id === assistantMessage.id)
            if (messageIndex !== -1) {
              messages.value[messageIndex] = {
                ...messages.value[messageIndex],
                content: accumulatedContent
              }
            }
            
            // å¤–éƒ¨ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ãŒã‚ã‚Œã°å‘¼ã³å‡ºã—
            onToken?.(chunk)
          },
          onToolCall: (toolCall: any) => {
            console.log('ğŸ› ï¸ Tool call detected during streaming:', toolCall)
            const execution = startToolExecution(toolCall)
            updateToolExecutionStatus(execution.id, 'running')
          },
          onToolResult: (result: any) => {
            console.log('ğŸ¯ Tool result received during streaming:', result)
            
            // å¯¾å¿œã™ã‚‹ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚’å®Œäº†çŠ¶æ…‹ã«æ›´æ–°
            const execution = activeToolExecutions.value.find(exec => 
              exec.toolCall.function.name === result.function_name
            )
            if (execution) {
              updateToolExecutionStatus(execution.id, 'completed', result)
            }
          },
          onError: (error) => {
            console.error('ğŸš¨ MCP tools streaming error:', error)
            const errorMsg = error.message || 'Unknown streaming error'
            addSystemMessage(`MCPãƒ„ãƒ¼ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: ${errorMsg}`)
            isStreamingWithTools.value = false
          },
          onEnd: (data) => {
            console.log('âœ… MCP tools streaming ended')
            console.log('Final accumulated content:', accumulatedContent)
            
            // ä¼šè©±å±¥æ­´ã®æœ€é©åŒ–ãŒã‚ã£ãŸå ´åˆã¯é©ç”¨
            if (data?.optimized_conversation_history && data.optimized_conversation_history.length > 0) {
              console.log('ğŸ—‚ï¸ Server provided optimized conversation history for MCP tools streaming:', 
                data.optimized_conversation_history.length, 'messages')
            }
            
            isStreamingWithTools.value = false
          }
        }
      )
      
      console.log('MCP tools streaming message sent successfully')
      
    } catch (err) {
      const errorMessage = err instanceof Error ? err.message : 'Unknown error occurred'
      console.error('Error sending streaming message with MCP tools and config:', err)
      
      // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
      addSystemMessage(`MCPãƒ„ãƒ¼ãƒ«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: ${errorMessage}`)
      isStreamingWithTools.value = false
    }
  }

  // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’åœæ­¢
  function stopStreaming() {
    if (currentStreamController.value) {
      if (typeof currentStreamController.value === 'function') {
        currentStreamController.value()
      } else {
        currentStreamController.value.abort()
      }
      currentStreamController.value = null
    }
    isStreamingWithTools.value = false
  }

  return {
    isStreamingWithTools,
    currentStreamController,
    sendStreamingMessage,
    sendStreamingMessageWithConfig,
    sendStreamingMessageWithToolsAndConfig,
    stopStreaming
  }
}